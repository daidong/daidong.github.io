---
title: "Undergraduate Resarch Intiative (ITCS 3050) - Spring 2023"
collection: teaching
type: "Undergraduated Course"
permalink: /teaching/2023-spring-2
venue: "UNC-Charlotte, Computer Science Department"
date: 2023-01-01
location: "Charlotte, US"
---
# General Information

## Why are we doing this?
We want to encourage our Undergraudate Students to participate research as early as possible. So, we are looking for **Sophomore** and **Junior** students.

## Who are doing this?
Faculty members in the Parallel High-Performance Computing Lab (PHPC) are doing this. 

We are [Tyler Allen](https://webpages.charlotte.edu/~tallen93/), [Dong Dai](http://daidong.github.io), [Erik Saule](https://webpages.charlotte.edu/~esaule/public-website/welcome.html), and [Yonghong Yan](https://passlab.github.io/yanyh/). 

We and our Ph.D. students will be mentoring your research. In addition, Md. Hasanur Rashid (mrashid2@uncc.edu) will be our TA to interact with you during the semester.

## How can I participate?
1. Spend 2-mins to fill this Google Form to regiester: [Registration Form]().
2. We will contact you **next week** to know more about your background and interests.
3. We will register you in **Course ITCS 3050** in Spring 2023 for this course.

## How will the class go?
1. You will be matched to a research project.
2. You will work with specific faculty and group.
3. We will host occational seminiar talks.
4. **Flexible class time and location**.

## What are the Research Projects?
Our mentors are currently working on list of research projects listed below. You will have the choices to pick the projects you are interested and participlate. You can also propose your own projects, as long as your mentors agree. 

### Project 1: Fast GPU Programming!
GPU Applications using out-of-the-box computational libraries, such as BLAS, often expose inefficiencies due to loss of locality between kernels (i.e. functions). Applications have available data reuse that is lost due to the “synchronization barrier” between kernels. If we can unfold these kernels, we can use techniques (analytical evolving into compiler) to combine kernels, and then leverage locality.

### Project 2: Accelerate Key Algorithms!
Iteratively Reweighted Least Squares (IRLS) method for matrix factorization can be benifited by the parallel and distributed nature of the High-performance Computing (HPC) environment. We hope to integrate, test, and benchmark the implemtation of MatrixIRLS into HPC environment.

### Project 3: Visualize Memory Accesses!
Systems and architectural processes are often very difficult to comprehend and understand. They also do not lend themselves easily to visualization. This project involves developing visualizations that can help us create graphical representations of data access and memory migration over time for heterogeneous systems. These visualizations could improve the understandability of data flow, data migration, remote data access, and overall application access patterns for arbitrary applications.

### Project 4: Machine Learning-based Parameter Tuning!
This project aims to understand the general High-Performance Computing (HPC) environment and the CAPES framework. The student will learn the basics of the High-performance Computing (HPC) environment and Parallel File System (PFS), specifically Lustre. The student will attain proficiency in how to create and maintain an HPC cluster. The student will get to explore how parameter tuning works and acquire knowledge on how to conduct structured and in-depth evaluations.

### Project 5: Understand Job Execution in SuperComputers!
The goal of this project is to understand how Supercomputers or Cloud run jobs internally. For instance, how might different jobs be submitted to Supercomputers? How these jobs are scheduled to run? What has to be done to make sure jobs can run efficiently without wasting the hardware? These are interesting and critically important questions to answer. In this project, the student will get familiar with these topics in a quantitive way. We will use some high-fidelity simulators to study how jobs are executed in large-scale environments accurately and efficiently.


### Project 6: Bringing High Performance Computing to Gephi!
Gephi is an open source tool for interactive graph visualization and analysis. While Gephi is a useful tool, the analysis it provides usually has simple implementations leading to low performance and high response time, in particular for analysis of high complexity. In this project, we will develop a plugin for Gephi to enable users to leverage existing codes and techniques to bring high performance computing to analysts.


### Project 7: Smart Compilers!
One kind of redundant instructions are those that do not change the operands of the instruction, for example an instruction that adds or subtracts 0, or an instruction that multiplies or divides 1. In this project, the students will learn how to use tools to perform analysis of compiler internal representation (IR) or binary code of a program to identify those kinds of instructions. Then they will explore how to use tools to rewrite the IR program or binary code to remove those instructions.
